# AI project reviewer for Turing College 

This tool uses a Large Language Model (LLM) to automatically review Data Science, Machine Learning, Data Analysis, and general programming projects. It analyzes the repository and provides feedback based on the project requirements and other important aspects

## How to use 
1. You can simply visit this [link](https://turing-ai-reviewer.onrender.com)
2. On the right hand side, press "Repository" button and enter
the repository URL. The URL should be in the following format:
`https://github.com/username/projectname`
3. Make sure the following conditions are met:
   - The repository should be public. 
   - It must contain a task description file from Turing College:
      * The file should have a name consisting only of numbers (e.g., 115.ipynb or 225.md).
      * It should include the project description and requirements.
   - The project consists of files with the following extensions: 
     `.py`, `.ipynb`, `.md`, `.txt`, `.sql`
4. Click the "Start AI Review" button.
5. Then, click "Analyze" to run the review process.
6. You can ask follow-up questions to the AI based on the review.

## How it works
1. The repository is fetched and downloaded as a ZIP archive.
2. The ZIP file is extracted locally.
3. The app locates the Turing College task description file (named with numbers only).
4. The project requirements is extracted from the file using Regex 
5. The project description is extracted from that file using an LLM. 
6. The LLM enhances the extracted requirements to ensure clarity
7. For each file:
   * The LLM receives the file content, name, summary, and project requirements.
   * A carefully engineered prompt guides the LLM to provide in-depth analysis. 
8. All LLM-generated feedbacks, requirements, and summaries are combined.
9. A final project review is generated by the LLM.
10. Users can then ask follow-up questions based on the review.
11. Since the review alone may lack full context, 
a separate LLM determines whether additional file content 
is needed to answer the question.
12. Relevant files are then passed to the main LLM to provide a more accurate response.
13. This process ensures context is preserved, especially for large, multi-file projects.
14. The system is designed to be scalable and has future improvement potential.

## Future Improvements
* **Smarter Prompt Engineering**
    * Improve prompt design to generate even more accurate, detailed, and context-aware feedback from the LLM.
* **Solution-Based Comparison**
  * Integrate Turing College’s “Suggested Solution” files to compare student submissions and generate richer, more targeted feedback.
* **Autonomous Reasoning Agent**. Develop a reasoning agent that:
  * Understands the project’s goal and structure
  * Decides which files are relevant
  * Chooses the best analysis approach for each part
  * Orchestrates LLM calls for code, text, and task evaluation
  * Combines results into a comprehensive review
* **Code Execution & Debugging**
  * Enable the agent to run project code to detect runtime errors or bugs automatically.
* **Static Code Analysis Tools**
  * Integrate linters, type checkers, and other analysis tools to assess code quality and maintainability.
* **Plot Analysis via Vision-Language Models (VLMs)**
  * Use multimodal models to interpret and critique plots (e.g., line graphs, bar charts) generated in the project.
* **Expanded File Type Support**
  * Add compatibility for more file formats (e.g., .py, .csv, .txt, .r, etc.) to broaden analysis coverage.
* **Voice Interaction Support**
  * Introduce voice-based input and responses in addition to the current chat-based interface.
* **Improved RAG** 
  * Replace LLM-based file selection with a vector database to better retrieve relevant files for answering follow-up questions.
* File Chunking for Large Projects
  * Automatically split large files into manageable chunks to allow deeper, more scalable analysis without losing context.
* **Interactive Inline Feedback in Notebooks**
  * Instead of summarizing all feedback at the end, inject comments or suggestions directly into .ipynb cells or Markdown (similar to how GitHub PR reviews work).
* **Model Critique with Chain-of-Thought and Multi-Agent Collaboration**
  * Use multiple specialized agents:
    * One for reviewing code
    * One for evaluating data science methodology
    * One for assessing storytelling/communication
  * Combine their outputs using a final “review composer” agent.
* **Dependency & Environment Checks**
  * Parse requirements.txt, environment.yml, or pip freeze to:
    * Check for missing, outdated, or unnecessary dependencies
    * Suggest best practices (e.g., pinning versions)
* Personalized Learning Tips
  * Based on review results, suggest specific learning resources
